{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MLOps: Guía Completa con Ejemplos\n",
        "\n",
        "## 1. Introducción a MLOps\n",
        "\n",
        "```markdown\n",
        "# Introducción a MLOps\n",
        "\n",
        "Machine Learning Operations (MLOps) es un conjunto de prácticas que automatizan y simplifican los flujos de trabajo y despliegues de machine learning. Representa una cultura y práctica que unifica el desarrollo de aplicaciones ML (Dev) con el despliegue y operaciones de sistemas ML (Ops).\n",
        "\n",
        "El mercado de MLOps se proyecta a alcanzar los 10.4 mil millones de USD para 2028, creciendo a un impresionante CAGR del 28.6% desde 2022 hasta 2028. Aproximadamente el 80% de las organizaciones planean adoptar MLOps en los próximos dos años.\n",
        "```\n",
        "\n",
        "## 2. Componentes Principales de MLOps\n",
        "\n",
        "```markdown\n",
        "## Componentes Principales de MLOps\n",
        "\n",
        "| Componente | Descripción | Herramientas/Frameworks | Usos |\n",
        "|------------|-------------|-------------------------|------|\n",
        "| **Version Control** | Control de versiones que se extiende más allá del código para incluir datos y modelos | - Git (código)<br>- DVC (Data Version Control)<br>- lakeFS<br>- Pachyderm | - Trazabilidad de cambios<br>- Reproducibilidad<br>- Capacidad de rollback<br>- Colaboración eficiente |\n",
        "| **CI/CD** | Pipelines que automatizan pruebas y despliegue de modelos ML | - GitHub Actions<br>- GitLab CI/CD<br>- Jenkins<br>- CircleCI<br>- Azure Pipelines | - Validación y pruebas automatizadas<br>- Procesos de despliegue consistentes<br>- Ciclos de iteración más rápidos<br>- Reducción de errores humanos |\n",
        "| **Orchestration** | Coordinación y automatización de pasos en flujos de trabajo ML | - Airflow<br>- Prefect<br>- Dagster<br>- Luigi<br>- Metaflow<br>- Flyte<br>- Kubeflow Pipelines | - Programación y ejecución de componentes<br>- Gestión de dependencias<br>- Manejo de fallos y reintentos<br>- Monitoreo de ejecución |\n",
        "| **Compute** | Gestión de infraestructura para entrenamiento y servicio de modelos | - AWS<br>- Google Cloud<br>- Azure<br>- Servicios especializados de ML | - Entornos de entrenamiento distribuido para GPUs<br>- Recursos de cómputo escalables<br>- Optimización de costos<br>- Despliegues en nube híbrida |\n",
        "| **Serving** | Despliegue de modelos entrenados en producción | - TorchServe<br>- Seldon Core<br>- AWS SageMaker<br>- Contenedores Docker | - Despliegue containerizado<br>- Endpoints para inferencia en tiempo real y batch<br>- Gestión de API<br>- Manejo de versiones |\n",
        "| **Monitoring** | Sistemas para seguimiento de modelos desplegados | - Evidently AI<br>- Arize AI<br>- Fiddler<br>- Prometheus<br>- Grafana | - Detección de model drift y data drift<br>- Seguimiento de rendimiento<br>- Alertas<br>- Monitoreo de métricas del sistema |\n",
        "| **Metrics Registry** | Catálogo de modelos ML y sus versiones | - MLflow<br>- Weights & Biases<br>- DVC Studio | - Seguimiento de linaje y procedencia<br>- Registro de métricas de rendimiento<br>- Descubrimiento y compartición de modelos<br>- Promoción por etapas (dev, test, prod) |\n",
        "```\n",
        "\n",
        "## 3. Ejemplos de Código por Componente\n",
        "\n",
        "### 3.1 Version Control - DVC\n",
        "\n",
        "```python\n",
        "# Ejemplo de Version Control con DVC\n",
        "# ---------------------------------\n",
        "# Instalación: pip install dvc\n",
        "\n",
        "# Inicializar DVC en un proyecto\n",
        "!dvc init\n",
        "\n",
        "# Agregar un conjunto de datos grande a DVC\n",
        "!dvc add data/large_dataset.csv\n",
        "\n",
        "# Añadir información de seguimiento a Git\n",
        "!git add data/large_dataset.csv.dvc .gitignore\n",
        "\n",
        "# Hacer commit de cambios\n",
        "!git commit -m \"Añadir dataset grande mediante DVC\"\n",
        "\n",
        "# Ejemplo básico de uso de DVC en un flujo de trabajo\n",
        "import os\n",
        "\n",
        "# Crear un script simple de procesamiento de datos\n",
        "with open('process_data.py', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Cargar datos\n",
        "data = pd.read_csv('data/large_dataset.csv')\n",
        "\n",
        "# Procesar datos\n",
        "processed_data = data.dropna()\n",
        "processed_data['nueva_columna'] = np.log(processed_data['columna_existente'] + 1)\n",
        "\n",
        "# Guardar datos procesados\n",
        "processed_data.to_csv('data/processed_dataset.csv', index=False)\n",
        "\"\"\")\n",
        "\n",
        "# Crear un archivo de etapa DVC\n",
        "with open('dvc.yaml', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "stages:\n",
        "  process:\n",
        "    cmd: python process_data.py\n",
        "    deps:\n",
        "      - process_data.py\n",
        "      - data/large_dataset.csv\n",
        "    outs:\n",
        "      - data/processed_dataset.csv\n",
        "\"\"\")\n",
        "\n",
        "# Ejecutar la etapa\n",
        "!dvc repro\n",
        "\n",
        "# Ver el gráfico de dependencias\n",
        "!dvc dag\n",
        "```\n",
        "\n",
        "### 3.2 CI/CD - GitHub Actions\n",
        "\n",
        "```python\n",
        "# Ejemplo de archivo YAML para GitHub Actions\n",
        "# Guarda esto como .github/workflows/ml-pipeline.yml\n",
        "\n",
        "ci_cd_example = \"\"\"\n",
        "name: ML Model CI/CD\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches: [ main ]\n",
        "  pull_request:\n",
        "    branches: [ main ]\n",
        "\n",
        "jobs:\n",
        "  test-and-deploy:\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "    - uses: actions/checkout@v3\n",
        "    - name: Set up Python\n",
        "      uses: actions/setup-python@v4\n",
        "      with:\n",
        "        python-version: '3.10'\n",
        "    \n",
        "    - name: Install dependencies\n",
        "      run: |\n",
        "        python -m pip install --upgrade pip\n",
        "        pip install pytest scikit-learn pandas numpy mlflow\n",
        "        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n",
        "    \n",
        "    - name: Test with pytest\n",
        "      run: |\n",
        "        pytest tests/\n",
        "    \n",
        "    - name: Train model\n",
        "      run: |\n",
        "        python train_model.py\n",
        "    \n",
        "    - name: Evaluate model\n",
        "      run: |\n",
        "        python evaluate_model.py\n",
        "        \n",
        "    - name: Deploy model (if tests pass and on main)\n",
        "      if: success() && github.ref == 'refs/heads/main'\n",
        "      run: |\n",
        "        python deploy_model.py\n",
        "\"\"\"\n",
        "\n",
        "print(ci_cd_example)\n",
        "\n",
        "# Ejemplo simple de script de entrenamiento\n",
        "with open('train_model.py', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "# Generar datos de ejemplo\n",
        "X = np.random.rand(1000, 10)\n",
        "y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
        "\n",
        "# Dividir en train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenar modelo\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Guardar modelo\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "print(\"Modelo entrenado y guardado con éxito\")\n",
        "\"\"\")\n",
        "```\n",
        "\n",
        "### 3.3 Orchestration - Airflow\n",
        "\n",
        "```python\n",
        "# Ejemplo de DAG en Apache Airflow\n",
        "# -----------------------------\n",
        "\n",
        "# Instalar Airflow: pip install apache-airflow\n",
        "\n",
        "# Ejemplo de definición de DAG\n",
        "dag_example = \"\"\"\n",
        "from datetime import datetime, timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.sensors.filesystem import FileSensor\n",
        "\n",
        "# Funciones para las tareas\n",
        "def extract_data():\n",
        "    print(\"Extrayendo datos...\")\n",
        "    # Código para extraer datos de la fuente\n",
        "    with open('/tmp/data_extracted.txt', 'w') as f:\n",
        "        f.write('Datos extraídos')\n",
        "\n",
        "def transform_data():\n",
        "    print(\"Transformando datos...\")\n",
        "    # Código para transformar datos\n",
        "    with open('/tmp/data_transformed.txt', 'w') as f:\n",
        "        f.write('Datos transformados')\n",
        "\n",
        "def train_model():\n",
        "    print(\"Entrenando modelo...\")\n",
        "    # Código para entrenar modelo\n",
        "    with open('/tmp/model_trained.txt', 'w') as f:\n",
        "        f.write('Modelo entrenado')\n",
        "\n",
        "def evaluate_model():\n",
        "    print(\"Evaluando modelo...\")\n",
        "    # Código para evaluar modelo\n",
        "    with open('/tmp/model_evaluated.txt', 'w') as f:\n",
        "        f.write('Modelo evaluado')\n",
        "\n",
        "def deploy_model():\n",
        "    print(\"Desplegando modelo...\")\n",
        "    # Código para desplegar modelo\n",
        "    with open('/tmp/model_deployed.txt', 'w') as f:\n",
        "        f.write('Modelo desplegado')\n",
        "\n",
        "# Configuración del DAG\n",
        "default_args = {\n",
        "    'owner': 'mlops_team',\n",
        "    'depends_on_past': False,\n",
        "    'email': ['mlops@example.com'],\n",
        "    'email_on_failure': True,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "with DAG(\n",
        "    'ml_training_pipeline',\n",
        "    default_args=default_args,\n",
        "    description='Pipeline de entrenamiento de ML con Airflow',\n",
        "    schedule_interval=timedelta(days=1),\n",
        "    start_date=datetime(2025, 1, 1),\n",
        "    catchup=False,\n",
        "    tags=['ml', 'training'],\n",
        ") as dag:\n",
        "    \n",
        "    # Tareas\n",
        "    extract_task = PythonOperator(\n",
        "        task_id='extract_data',\n",
        "        python_callable=extract_data,\n",
        "    )\n",
        "    \n",
        "    # Sensor para esperar que los datos estén disponibles\n",
        "    wait_for_data = FileSensor(\n",
        "        task_id='wait_for_data',\n",
        "        filepath='/tmp/data_extracted.txt',\n",
        "        poke_interval=30,  # Revisar cada 30 segundos\n",
        "        timeout=60 * 5,  # Tiempo límite de 5 minutos\n",
        "    )\n",
        "    \n",
        "    transform_task = PythonOperator(\n",
        "        task_id='transform_data',\n",
        "        python_callable=transform_data,\n",
        "    )\n",
        "    \n",
        "    train_task = PythonOperator(\n",
        "        task_id='train_model',\n",
        "        python_callable=train_model,\n",
        "    )\n",
        "    \n",
        "    evaluate_task = PythonOperator(\n",
        "        task_id='evaluate_model',\n",
        "        python_callable=evaluate_model,\n",
        "    )\n",
        "    \n",
        "    deploy_task = PythonOperator(\n",
        "        task_id='deploy_model',\n",
        "        python_callable=deploy_model,\n",
        "    )\n",
        "    \n",
        "    # Definir el flujo de ejecución\n",
        "    extract_task >> wait_for_data >> transform_task >> train_task >> evaluate_task >> deploy_task\n",
        "\"\"\"\n",
        "\n",
        "print(dag_example)\n",
        "```\n",
        "\n",
        "### 3.4 Compute - Ejemplo de configuración con AWS\n",
        "\n",
        "```python\n",
        "# Ejemplo de configuración de infraestructura AWS para ML con Boto3\n",
        "# -----------------------------------------------------------------\n",
        "# Instalación: pip install boto3\n",
        "\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "# Ejemplo básico de configuración de instancia EC2 para ML\n",
        "def create_ml_ec2_instance():\n",
        "    ec2 = boto3.resource('ec2')\n",
        "    \n",
        "    # Crear una instancia con GPU para entrenamiento\n",
        "    instances = ec2.create_instances(\n",
        "        ImageId='ami-0c55b159cbfafe1f0',           # AMI con software ML preinstalado\n",
        "        InstanceType='p3.2xlarge',                  # Instancia con GPU para ML\n",
        "        MinCount=1,\n",
        "        MaxCount=1,\n",
        "        KeyName='my-key-pair',                      # Tu key pair\n",
        "        SecurityGroupIds=['sg-12345678'],           # Tu security group\n",
        "        BlockDeviceMappings=[\n",
        "            {\n",
        "                'DeviceName': '/dev/sda1',\n",
        "                'Ebs': {\n",
        "                    'VolumeSize': 100,              # 100GB para datasets y modelos\n",
        "                    'DeleteOnTermination': True,\n",
        "                    'VolumeType': 'gp2',\n",
        "                }\n",
        "            },\n",
        "        ],\n",
        "        TagSpecifications=[\n",
        "            {\n",
        "                'ResourceType': 'instance',\n",
        "                'Tags': [\n",
        "                    {\n",
        "                        'Key': 'Name',\n",
        "                        'Value': 'ML-Training-Instance'\n",
        "                    },\n",
        "                    {\n",
        "                        'Key': 'Project',\n",
        "                        'Value': 'Customer-Churn-Prediction'\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        UserData='''#!/bin/bash\n",
        "                    pip install -U scikit-learn tensorflow pytorch\n",
        "                    git clone https://github.com/company/ml-project.git\n",
        "                    cd ml-project\n",
        "                    python setup.py install\n",
        "                 '''\n",
        "    )\n",
        "    \n",
        "    print(f\"Instancia creada con ID: {instances[0].id}\")\n",
        "    return instances[0].id\n",
        "\n",
        "# Ejemplo de configuración de SageMaker para entrenamiento\n",
        "def create_sagemaker_training_job():\n",
        "    sagemaker = boto3.client('sagemaker')\n",
        "    \n",
        "    training_job_name = 'churn-prediction-xgboost-' + str(int(time.time()))\n",
        "    \n",
        "    # Configurar un trabajo de entrenamiento de SageMaker\n",
        "    response = sagemaker.create_training_job(\n",
        "        TrainingJobName=training_job_name,\n",
        "        AlgorithmSpecification={\n",
        "            'TrainingImage': '123456789012.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
        "            'TrainingInputMode': 'File'\n",
        "        },\n",
        "        RoleArn='arn:aws:iam::123456789012:role/SageMakerRole',\n",
        "        InputDataConfig=[\n",
        "            {\n",
        "                'ChannelName': 'train',\n",
        "                'DataSource': {\n",
        "                    'S3DataSource': {\n",
        "                        'S3DataType': 'S3Prefix',\n",
        "                        'S3Uri': 's3://my-bucket/training-data/',\n",
        "                        'S3DataDistributionType': 'FullyReplicated'\n",
        "                    }\n",
        "                },\n",
        "                'ContentType': 'csv',\n",
        "                'CompressionType': 'None'\n",
        "            }\n",
        "        ],\n",
        "        OutputDataConfig={\n",
        "            'S3OutputPath': 's3://my-bucket/output/'\n",
        "        },\n",
        "        ResourceConfig={\n",
        "            'InstanceType': 'ml.m5.xlarge',\n",
        "            'InstanceCount': 1,\n",
        "            'VolumeSizeInGB': 50\n",
        "        },\n",
        "        StoppingCondition={\n",
        "            'MaxRuntimeInSeconds': 86400\n",
        "        },\n",
        "        HyperParameters={\n",
        "            'max_depth': '6',\n",
        "            'eta': '0.2',\n",
        "            'objective': 'binary:logistic',\n",
        "            'num_round': '100'\n",
        "        },\n",
        "        Tags=[\n",
        "            {\n",
        "                'Key': 'Project',\n",
        "                'Value': 'CustomerChurn'\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    print(f\"Trabajo de entrenamiento creado: {training_job_name}\")\n",
        "    return response\n",
        "\n",
        "# Estos son ejemplos y requerirían credenciales AWS configuradas para ejecutarse\n",
        "# print(\"Ejemplos de configuración AWS para ML\")\n",
        "```\n",
        "\n",
        "### 3.5 Model Serving - Flask API para servir un modelo\n",
        "\n",
        "```python\n",
        "# Ejemplo de Servicio de Modelo con Flask\n",
        "# --------------------------------------\n",
        "# Instalación: pip install flask scikit-learn pandas\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "# Crear una aplicación Flask\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Cargar un modelo pre-entrenado (simulado aquí)\n",
        "def load_model():\n",
        "    # En un caso real, cargarías un modelo previamente entrenado\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    # Entrenamiento simulado\n",
        "    X = np.random.rand(1000, 4)\n",
        "    y = (X[:, 0] + X[:, 1] > 1).astype(int)\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "# Cargar el modelo al iniciar\n",
        "model = load_model()\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    \"\"\"Endpoint para realizar predicciones.\"\"\"\n",
        "    # Obtener datos de la solicitud\n",
        "    data = request.json\n",
        "    \n",
        "    # Verificar que los datos tengan el formato esperado\n",
        "    if not data or 'features' not in data:\n",
        "        return jsonify({'error': 'No features provided'}), 400\n",
        "    \n",
        "    try:\n",
        "        # Convertir datos a numpy array\n",
        "        features = np.array(data['features'])\n",
        "        \n",
        "        # Hacer predicción\n",
        "        prediction = model.predict(features.reshape(1, -1))\n",
        "        prediction_proba = model.predict_proba(features.reshape(1, -1)).tolist()\n",
        "        \n",
        "        # Devolver respuesta\n",
        "        return jsonify({\n",
        "            'prediction': int(prediction[0]),\n",
        "            'probability': prediction_proba[0]\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    \"\"\"Endpoint para verificar la salud del servicio.\"\"\"\n",
        "    return jsonify({'status': 'healthy'})\n",
        "\n",
        "# Código para ejecutar la aplicación\n",
        "if __name__ == '__main__':\n",
        "    print(\"Iniciando servidor de predicción en http://localhost:5000\")\n",
        "    print(\"Endpoints disponibles:\")\n",
        "    print(\"  - POST /predict - Para realizar predicciones\")\n",
        "    print(\"  - GET /health - Para verificar la salud del servicio\")\n",
        "    # app.run(host='0.0.0.0', port=5000)\n",
        "    \n",
        "# Ejemplo de solicitud:\n",
        "example_request = {\n",
        "    \"features\": [0.5, 0.8, 0.2, 0.1]\n",
        "}\n",
        "print(f\"\\nEjemplo de solicitud: {example_request}\")\n",
        "```\n",
        "\n",
        "### 3.6 Monitoring - Monitoreo de Modelos con Evidently\n",
        "\n",
        "```python\n",
        "# Ejemplo de Monitoreo de Modelos con Evidently\n",
        "# -------------------------------------------\n",
        "# Instalación: pip install evidently pandas scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from evidently.report import Report\n",
        "from evidently.metric_preset import DataDriftPreset, TargetDriftPreset, RegressionPreset\n",
        "\n",
        "# Generar datos de ejemplo\n",
        "def generate_data():\n",
        "    # Cargar datos de diabetes\n",
        "    diabetes = load_diabetes()\n",
        "    X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "    y = diabetes.target\n",
        "    \n",
        "    # Dividir en train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "    # Entrenar un modelo\n",
        "    model = RandomForestRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Crear dataset de referencia (training)\n",
        "    reference_data = X_train.copy()\n",
        "    reference_data['target'] = y_train\n",
        "    \n",
        "    # Crear dataset actual (test)\n",
        "    current_data = X_test.copy()\n",
        "    current_data['target'] = y_test\n",
        "    \n",
        "    # Agregar predicciones\n",
        "    reference_data['prediction'] = model.predict(X_train)\n",
        "    current_data['prediction'] = model.predict(X_test)\n",
        "    \n",
        "    return reference_data, current_data\n",
        "\n",
        "# Crear un informe de monitoreo\n",
        "def create_monitoring_report(reference_data, current_data):\n",
        "    # Reporte de Data Drift\n",
        "    data_drift_report = Report(metrics=[DataDriftPreset()])\n",
        "    data_drift_report.run(reference_data=reference_data, current_data=current_data)\n",
        "    data_drift_json = data_drift_report.json()\n",
        "    \n",
        "    # Reporte de Target Drift\n",
        "    target_drift_report = Report(metrics=[TargetDriftPreset()])\n",
        "    target_drift_report.run(reference_data=reference_data, current_data=current_data)\n",
        "    target_drift_json = target_drift_report.json()\n",
        "    \n",
        "    # Reporte de Rendimiento de Regresión\n",
        "    regression_report = Report(metrics=[RegressionPreset()])\n",
        "    regression_report.run(reference_data=reference_data, current_data=current_data,\n",
        "                          column_mapping={'target': 'target', 'prediction': 'prediction'})\n",
        "    regression_json = regression_report.json()\n",
        "    \n",
        "    return {\n",
        "        'data_drift': data_drift_json,\n",
        "        'target_drift': target_drift_json,\n",
        "        'regression_performance': regression_json\n",
        "    }\n",
        "\n",
        "# Generar datos y crear informe\n",
        "reference_data, current_data = generate_data()\n",
        "report = create_monitoring_report(reference_data, current_data)\n",
        "\n",
        "print(\"Informe de monitoreo generado. El informe contiene métricas de:\")\n",
        "print(\"1. Data Drift - Detecta cambios en la distribución de las features\")\n",
        "print(\"2. Target Drift - Detecta cambios en la distribución del target\")\n",
        "print(\"3. Regression Performance - Evalúa el rendimiento del modelo de regresión\")\n",
        "\n",
        "# En un caso real, guardarías estos informes o los enviarías a un dashboard\n",
        "```\n",
        "\n",
        "### 3.7 Metrics Registry - MLflow\n",
        "\n",
        "```python\n",
        "# Ejemplo de Metrics Registry con MLflow\n",
        "# -----------------------------------\n",
        "# Instalación: pip install mlflow scikit-learn pandas matplotlib\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Configurar el servidor MLflow\n",
        "# En producción, configurarías un servidor centralizado\n",
        "# mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "\n",
        "# Crear un experimento\n",
        "mlflow.set_experiment(\"customer_churn_prediction\")\n",
        "\n",
        "# Generar datos de ejemplo\n",
        "def generate_sample_data():\n",
        "    # Datos simulados de clientes\n",
        "    n_samples = 1000\n",
        "    X = np.random.rand(n_samples, 5)  # 5 características\n",
        "    # Crear una relación para y basada en las características\n",
        "    y = (X[:, 0] > 0.7) | ((X[:, 1] > 0.5) & (X[:, 2] < 0.3))\n",
        "    y = y.astype(int)  # Convertir a 0 y 1\n",
        "    \n",
        "    # Convertir a DataFrame para mejor manejo\n",
        "    feature_names = ['usage_time', 'num_complaints', 'subscription_length', 'age', 'num_services']\n",
        "    X_df = pd.DataFrame(X, columns=feature_names)\n",
        "    \n",
        "    return X_df, y\n",
        "\n",
        "# Entrenar y registrar modelos\n",
        "def train_and_log_model():\n",
        "    # Generar datos\n",
        "    X, y = generate_sample_data()\n",
        "    \n",
        "    # Dividir en training y test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "    # Iniciar el run de MLflow\n",
        "    with mlflow.start_run(run_name=\"rf_model_run\") as run:\n",
        "        # Registrar parámetros\n",
        "        params = {\n",
        "            \"n_estimators\": 100,\n",
        "            \"max_depth\": 5,\n",
        "            \"min_samples_split\": 2,\n",
        "            \"random_state\": 42\n",
        "        }\n",
        "        mlflow.log_params(params)\n",
        "        \n",
        "        # Entrenar modelo\n",
        "        rf = RandomForestClassifier(**params)\n",
        "        rf.fit(X_train, y_train)\n",
        "        \n",
        "        # Realizar predicciones\n",
        "        y_pred = rf.predict(X_test)\n",
        "        \n",
        "        # Calcular métricas\n",
        "        metrics = {\n",
        "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "            \"precision\": precision_score(y_test, y_pred),\n",
        "            \"recall\": recall_score(y_test, y_pred),\n",
        "            \"f1\": f1_score(y_test, y_pred)\n",
        "        }\n",
        "        \n",
        "        # Registrar métricas\n",
        "        mlflow.log_metrics(metrics)\n",
        "        \n",
        "        # Registrar modelo\n",
        "        mlflow.sklearn.log_model(rf, \"random_forest_model\",\n",
        "                                registered_model_name=\"churn_prediction_model\")\n",
        "        \n",
        "        # Registrar feature importance como una figura\n",
        "        feature_importance = pd.DataFrame(\n",
        "            rf.feature_importances_,\n",
        "            index=X.columns,\n",
        "            columns=['importance']\n",
        "        ).sort_values('importance', ascending=False)\n",
        "        \n",
        "        # En un caso real, aquí generarías y guardarías una figura\n",
        "        # fig = feature_importance.plot(kind='bar', figsize=(10, 6)).get_figure()\n",
        "        # fig.savefig(\"feature_importance.png\")\n",
        "        # mlflow.log_artifact(\"feature_importance.png\")\n",
        "        \n",
        "        # Registrar ejemplo de datos de entrada\n",
        "        mlflow.log_input(X_train, context=\"training\")\n",
        "        \n",
        "        print(f\"Modelo entrenado y registrado con MLflow - Run ID: {run.info.run_id}\")\n",
        "        print(f\"Métricas: {metrics}\")\n",
        "        print(f\"El modelo ha sido registrado como 'churn_prediction_model'\")\n",
        "        \n",
        "        return run.info.run_id, metrics\n",
        "\n",
        "# Ejecutar el entrenamiento y registro\n",
        "run_id, metrics = train_and_log_model()\n",
        "\n",
        "# Mostrar cómo cargar un modelo registrado\n",
        "print(\"\\nPara cargar el modelo registrado:\")\n",
        "print(\"model = mlflow.sklearn.load_model('models:/churn_prediction_model/latest')\")\n",
        "\n",
        "# Mostrar cómo usar el modelo para predicciones\n",
        "print(\"\\nEjemplo de uso para predicciones:\")\n",
        "print(\"\"\"\n",
        "import mlflow.sklearn\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar el modelo registrado\n",
        "model = mlflow.sklearn.load_model('models:/churn_prediction_model/latest')\n",
        "\n",
        "# Preparar datos para predicción\n",
        "new_data = pd.DataFrame({\n",
        "    'usage_time': [0.8],\n",
        "    'num_complaints': [0.2],\n",
        "    'subscription_length': [0.6],\n",
        "    'age': [0.4],\n",
        "    'num_services': [0.7]\n",
        "})\n",
        "\n",
        "# Hacer predicción\n",
        "prediction = model.predict(new_data)\n",
        "print(f'Predicción: {prediction}')\n",
        "\"\"\")\n",
        "```\n",
        "\n",
        "## 4. Flujo de Trabajo de MLOps\n",
        "\n",
        "```markdown\n",
        "## Flujo de Trabajo de MLOps\n",
        "\n",
        "| Etapa | Descripción | Herramientas Destacadas | Mejores Prácticas |\n",
        "|-------|-------------|-------------------------|-------------------|\n",
        "| **Ingesta de Datos** | Recopilación y validación de datos | - Feast<br>- Tecton<br>- Hopsworks | - Implementar verificaciones de calidad<br>- Aplicar versionado<br>- Establecer políticas de gobernanza |\n",
        "| **Preparación de Datos** | Limpieza, transformación y feature engineering | - Pandas<br>- PySpark<br>- Feature Stores | - Estandarizar pipelines de datos<br>- Automatizar transformaciones<br>- Documentar metadatos |\n",
        "| **Entrenamiento de Modelos** | Desarrollo y ajuste de modelos | - PyTorch<br>- TensorFlow<br>- Scikit-learn<br>- MLflow | - Implementar validación cruzada<br>- Aplicar CI/CD para entrenamiento<br>- Mantener reproducibilidad |\n",
        "| **Evaluación de Modelos** | Verificación del rendimiento del modelo | - MLflow<br>- Weights & Biases<br>- Neptune.ai | - Definir métricas claras<br>- Comparar con líneas base<br>- Validar con datos representativos |\n",
        "| **Registro de Modelos** | Catalogación y control de versiones de modelos | - MLflow Model Registry<br>- DVC Studio | - Mantener metadatos completos<br>- Implementar políticas de aprobación<br>- Establecer etiquetas de versión |\n",
        "| **Despliegue** | Puesta en producción de modelos | - Docker<br>- Kubernetes<br>- Seldon Core<br>- BentoML | - Usar contenedores<br>- Implementar despliegues blue-green<br>- Automatizar pruebas de integración |\n",
        "| **Monitoreo** | Seguimiento del rendimiento y alertas | - Evidently AI<br>- Prometheus<br>- Grafana | - Monitorear drift del modelo<br>- Establecer alertas proactivas<br>- Implementar paneles de control |\n",
        "| **Reentrenamiento** | Actualización periódica de modelos | - Airflow<br>- Kubeflow<br>- MLflow | - Definir disparadores de reentrenamiento<br>- Automatizar evaluación/despliegue<br>- Mantener historial de rendimiento |\n",
        "```\n",
        "\n",
        "## 5. Tendencias Emergentes en MLOps\n",
        "\n",
        "```markdown\n",
        "## Tendencias Emergentes en MLOps\n",
        "\n",
        "| Tendencia | Descripción | Tecnologías Relacionadas |\n",
        "|-----------|-------------|--------------------------|\n",
        "| **LLMOps** | Prácticas especializadas para modelos de lenguaje grandes | - Frameworks de fine-tuning<br>- Herramientas de prompt engineering<br>- Sistemas RAG |\n",
        "| **Edge MLOps** | Despliegue y gestión de modelos en dispositivos edge | - TensorFlow Lite<br>- ONNX Runtime<br>- Edge ML frameworks |\n",
        "| **MLOps Automatizado** | Mayor automatización de flujos de trabajo ML | - AutoML<br>- Hyperparameter tuning automatizado<br>- Feature engineering automatizado |\n",
        "| **MLOps para IA Responsable** | Integración de consideraciones éticas en flujos de trabajo | - Fairness indicators<br>- Herramientas de explicabilidad<br>- Frameworks de cumplimiento |\n",
        "| **Prácticas de IA Sostenible** | Enfoque en eficiencia energética y sostenibilidad | - Técnicas de entrenamiento eficientes<br>- Optimización de modelos<br>- Métricas de huella de carbono |\n",
        "```\n",
        "\n",
        "### 5.1 Ejemplo de LLMOps - Fine-tuning un modelo para RAG\n",
        "\n",
        "```python\n",
        "# Ejemplo de LLMOps - Retrieval Augmented Generation (RAG)\n",
        "# --------------------------------------------------------\n",
        "# Instalación: pip install langchain openai\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "n2AUWskk9_LQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7uWNqalH-Z4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Componentes Principales de MLOps\n",
        "\n",
        "| Componente | Descripción | Herramientas/Frameworks | Usos |\n",
        "|------------|-------------|-------------------------|------|\n",
        "| **Version Control** | Control de versiones que se extiende más allá del código para incluir datos y modelos | - Git (código)<br>- DVC (Data Version Control)<br>- lakeFS<br>- Pachyderm | - Trazabilidad de cambios<br>- Reproducibilidad<br>- Capacidad de rollback<br>- Colaboración eficiente |\n",
        "| **CI/CD** | Pipelines que automatizan pruebas y despliegue de modelos ML | - GitHub Actions<br>- GitLab CI/CD<br>- Jenkins<br>- CircleCI<br>- Azure Pipelines | - Validación y pruebas automatizadas<br>- Procesos de despliegue consistentes<br>- Ciclos de iteración más rápidos<br>- Reducción de errores humanos |\n",
        "| **Orchestration** | Coordinación y automatización de pasos en flujos de trabajo ML | - Airflow<br>- Prefect<br>- Dagster<br>- Luigi<br>- Metaflow<br>- Flyte<br>- Kubeflow Pipelines | - Programación y ejecución de componentes<br>- Gestión de dependencias<br>- Manejo de fallos y reintentos<br>- Monitoreo de ejecución |\n",
        "| **Compute** | Gestión de infraestructura para entrenamiento y servicio de modelos | - AWS<br>- Google Cloud<br>- Azure<br>- Servicios especializados de ML | - Entornos de entrenamiento distribuido para GPUs<br>- Recursos de cómputo escalables<br>- Optimización de costos<br>- Despliegues en nube híbrida |\n",
        "| **Serving** | Despliegue de modelos entrenados en producción | - TorchServe<br>- Seldon Core<br>- AWS SageMaker<br>- Contenedores Docker | - Despliegue containerizado<br>- Endpoints para inferencia en tiempo real y batch<br>- Gestión de API<br>- Manejo de versiones |\n",
        "| **Monitoring** | Sistemas para seguimiento de modelos desplegados | - Evidently AI<br>- Arize AI<br>- Fiddler<br>- Prometheus<br>- Grafana | - Detección de model drift y data drift<br>- Seguimiento de rendimiento<br>- Alertas<br>- Monitoreo de métricas del sistema |\n",
        "| **Metrics Registry** | Catálogo de modelos ML y sus versiones | - MLflow<br>- Weights & Biases<br>- DVC Studio | - Seguimiento de linaje y procedencia<br>- Registro de métricas de rendimiento<br>- Descubrimiento y compartición de modelos<br>- Promoción por etapas (dev, test, prod) |"
      ],
      "metadata": {
        "id": "rGaqNwHt-YL6"
      }
    }
  ]
}